---
title: "[ê³ ë¬¸ì„œë³µì›] Partial Convolutions ë…¼ë¬¸ & ì½”ë“œ ë¦¬ë·° "
excerpt: "2019. 08. 03. Partial Convolution"
search: true
categories: 
  - restoration

---

## Image Inpainting for Irregular Holes Using Partial Convolutions

[ë…¼ë¬¸ë§í¬](https://arxiv.org/pdf/1804.07723.pdf)

[code](https://github.com/MathiasGruber/PConv-Keras/blob/master/libs/pconv_model.py)

------

#### 1. ì´ˆë¡

<center>í‚¤ì›Œë“œ : ì´ë¯¸ì§€ ì¸í˜ì¸íŒ…, íŒŒì…œ ì½˜ë³¼ë£¨ì…˜  </center>

ì´ë¯¸ì§€ ë¹ˆì¹¸ ì±„ìš°ê¸°ë¥¼ ìœ„í•´ ì „í†µì  Convolutionì´ ì•„ë‹Œ partial convolution ì œì•ˆí•œë‹¤. convolutionì´ ìœ íš¨ í”½ì…€ì—ë§Œ ì ìš©ë˜ë„ë¡ ì œí•œí•˜ëŠ” ì–´ë–¤ ë§ˆìŠ¤í‚¹ì„ ê±°ì¹˜ê²Œ í•¨. ì´í›„ ì¬ ë…¸ë©€ë¼ì´ì¦ˆëœ ê²ƒì´ partial -. í¬ì›Œë“œ íŒ¨ìŠ¤ì˜ í•œ ë¶€ë¶„ìœ¼ë¡œì„œ í˜„ ë ˆì´ì–´ì˜ ë‹¤ìŒ ë ˆì´ì–´ë¡œ ê°ˆ ë•Œ ì—…ë°ì´íŠ¸ëœ ë§ˆìŠ¤í¬ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ê²Œ í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ë„ ì œì•ˆí•œë‹¤.



#### 2. Related Work

- Non-learning Based model
  1. ì´ì›ƒí•œ í”½ì…€ì„ ì´ìš©í•˜ì—¬ ì±„ì›Œ ë„£ëŠ” ê¸°ë²• 
  2. êµ¬ë©ì´ ì‘ì€ ê²½ìš°ì—ë§Œ ê°€ëŠ¥
  3. textureì˜ varianceê°€ ì‘ì•„ì•¼ ê°€ëŠ¥ 
  4. computing costê°€ ë§¤ìš° í¼ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì–´ë ¤ì›€ 
- Deep learning Based model 
  1. ì¼ì •í•œ placeholder ê°’ì„ ê°€ì§€ê³  holeì˜ ê°’ì„ ì´ˆê¸°í™”í•˜ëŠ”ê²Œ ì¼ë°˜ì ì„ë…¼ë¬¸ì—ì„œëŠ” í¬ê²Œ context encoderì™€ Semantic Image Inpainting with Deep Generative Model ì†Œê°œ 



#### 3. Partial Convolutional Layer

- **input : ì›ë³¸ ì´ë¯¸ì§€, ë§ˆìŠ¤í¬ëœ ì´ë¯¸ì§€**

- **output : output feature & output mask** 

  

![image](https://user-images.githubusercontent.com/26568793/62411617-8fdfb900-b630-11e9-8c2a-35f2e3c92c83.png)



- **xâ€™ : output feature**

![image](https://user-images.githubusercontent.com/26568793/62411693-282a6d80-b632-11e9-9baf-0fdcaab81c91.png)

1. W: convolution filter weights

2. b: bias

3. X : picture values (inputìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” feature) 

4. M : binary Mask (inputìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” Mask - ìš°ë¦¬ê°€ ë§Œë“¤ê²Œ ë˜ëŠ” hole)  

   â†’ hole vs non hole : hole ì€ maskëœ ë¶€ë¶„ìœ¼ë¡œ binary ê°’ìœ¼ë¡œ 0                                                                      non-holeì€ ë§ˆìŠ¤í¬ ë˜ì§€ ì•Šì€ ë¶€ë¶„ìœ¼ë¡œ binary ê°’ì€ 1

   

<p align="center">
   <img src="https://user-images.githubusercontent.com/26568793/62411915-91f84680-b635-11e9-94e5-1a36ef4c610b.png">
</p>

<p align="center">[í°ìƒ‰ì´ non-hole (1)  / ê²€ì€ ë¶€ë¶„ì´ hole(0) ]</p>

slideí•˜ë©° convolutionì„ ì§„í–‰í•  ë•Œ  non-holeì¸ ë¶€ë¶„ì´ ì¡°ê¸ˆì´ë¼ë„ ìˆë‹¤ë©´ ìƒìœ„ì˜ ì—°ì‚°ì„ ì§„í–‰,  (ì¦‰, ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ê°€ ë™ì‹œì— ë“¤ì–´ì˜¬ ë•Œ) convolutionì— ëª¨ë‘ ë§ˆìŠ¤í¬ë§Œ ë“¤ì–´ì™€ ìˆë‹¤ë©´ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•Šê³  pass í•¨. 

<p align="center">
   <img src="https://user-images.githubusercontent.com/26568793/62411929-bf44f480-b635-11e9-9ed5-dd42d784a40d.png">
</p>

í•´ë‹¹ ì—°ì‚°ì„ ë°˜ë³µì ìœ¼ë¡œ ì§„í–‰í•¨ì— ë”°ë¼ holeì¸ ë¶€ë¶„ì´ ìƒìœ„ ì—°ì‚°ìœ¼ë¡œ ì±„ì›Œì§€ê¸° ë•Œë¬¸ì— ì ì  holeì´ ì±„ì›Œì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ. 

sum(1) / sum(M)  (scaling factor) : ë§ˆìŠ¤í¬ê°€ ì“°ì´ì§€ ì•Šì€ ë¶€ë¶„ì˜ í”½ì…€ì— ê³±í•´ì„œ ì¡°ì •. í”½ì…€ ê°’ì˜ ë…¸ë§ë¼ì´ì¦ˆë¥¼ ë‹´ë‹¹.

- **mâ€™ : output mask**

<p align="center">
   <img src="https://user-images.githubusercontent.com/26568793/62411950-059a5380-b636-11e9-83d8-40afc845b332.png">
</p>

sum(M) > 0 : hole ì•„ë‹Œ ë¶€ë¶„ì´ 1ê°œë¼ë„ í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš° ì±„ì›Œì§.

- **partial conv ì´í•´** 

<p align="center">
   <img src="https://user-images.githubusercontent.com/26568793/62411958-295d9980-b636-11e9-95cf-b53ea29726cb.png">
</p>

#### 3.2 Networkì˜ ì ìš© 

Binary Maskì˜ ê²½ìš° ì‚¬ì´ì¦ˆê°€ CxHxW : image ì˜ í”¼ì³ ì‚¬ì´ì¦ˆì™€ ë™ì¼, ê³ ì •ëœ conv layerë¥¼ í†µí•´ì„œ ë™ì¼í•œ kernel ì‚¬ì´ì¦ˆì˜ Pconv êµ¬í˜„. Netwokì˜ ê²½ìš° U-net baseì„ì„ ì„¤ëª…

<p align="center">
   <img src="https://user-images.githubusercontent.com/26568793/62411982-793c6080-b636-11e9-88f1-7e379c592b56.png">
</p>

- 3 Channelì˜ ë§ˆìŠ¤í¬ì™€ ì´ë¯¸ì§€ë¡œ êµ¬ì„± 
- ì¢Œìš° : partical conv / ìƒí•˜ : batch norm
- ë§ˆì§€ë§‰ Decoder ë‹¨ê³„ì— input ì´ë¯¸ì§€ë¥¼ concatí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆëŠ”ë°, ì´ ê³¼ì •ì˜     ì˜ë¯¸ëŠ” inputì´ë¯¸ì§€ì˜ ì •ë³´ë¥¼ output ì´ë¯¸ì§€ì— ì „ë‹¬í•˜ê¸° ìœ„í•¨.
- Nearest neighbor up-sampling ì´ìš© 
- ì´ë¯¸ì§€ ë°”ìš´ë”ë¦¬ì— Pconvì™€ í•¨ê»˜ typicalí•œ Pconvë¥¼ í–‰í–ˆìŒì„ ì„¤ëª…, ì´ë¯¸ì§€ ë°–ì˜ í”½ì…€ë“¤ì´ ë˜ë‹¤ë¥¸ êµ¬ë©ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ì•Šê²Œ ì£¼ì˜



#### 3.3 Loss function 

Loss í•¨ìˆ˜ì˜ ëª©ì ì€ pixelë‹¨ìœ„ì˜ ë³µì› ì •í™•ë„ì™€ ì£¼ë³€ê³¼ êµ¬ë©ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ì§€ì— ëŒ€í•œ ê²ƒë“¤ì´ë‹¤. total LossëŠ” ê°ì¢… Lossë“¤ì˜ ì¡°í•©ë“¤ì´ë‹¤. í•˜ë‚˜ì”© ì‚´í´ë³´ì.

<p align="center">
   <img src="https://user-images.githubusercontent.com/26568793/62412023-2b742800-b637-11e9-8345-158f47b4149d.png">
</p>

- Iin  : input image with hole

- Iout  : output feature(image), network prediction(output) (ëª¨ë¸ì´ ë‚¸ ê²°ê³¼)

- M: initial binary mask
- Igt  : Ground truth image
- Icomp  = Iout but with the non-hole pixels directly set to ground truth
- ğ›¹(í¬ì‚¬ì´) : activation map of the nth selected layer



- **ì´ 6ê°œì˜ Lossë¥¼ ì¡°í•©** 

  ```python
  # ì´ˆê¸° ê°’ 
  import torch
  import os
  import torch.nn as nn
  import numpy as np
  
  from torchvision import models
  from torchvision import transforms
  from places2_train import Places2Data, MEAN, STDDEV
  from PIL import Image
  
  LAMBDAS = {"valid": 1.0, "hole": 6.0, "tv": 2.0, "perceptual": 0.05, "style": 240.0} # ê°ê°ì˜ lossë“¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ 
  ```

  

  1.  **L valid : non hole loss**

     ![image](https://user-images.githubusercontent.com/26568793/62412141-552e4e80-b639-11e9-9bfa-c888a3419846.png)

     Non-holeì¸ ë¶€ë¶„ì— ëŒ€í•œ loss ê³„ì‚°ë³µì›í•œ ì´ë¯¸ì§€, ì›ë³¸ì´ë¯¸ì§€(ì†ìƒë˜ì§€ ì•Šì€ ì´ë¯¸ì§€) â†’ ì†ìƒë˜ì§€ ì•Šì€ ë¶€ë¶„ì— ëŒ€í•œ loss ê°’

  2.  **L hole : hole loss**

     ![image](https://user-images.githubusercontent.com/26568793/62412150-860e8380-b639-11e9-906b-18cde80ae6b6.png)

     hole ì¸ ë¶€ë¶„ì— ëŒ€í•œ loss ê³„ì‚°ì†ìƒëœ ë¶€ë¶„ì„ ì–¼ë§ˆë‚˜ ì˜ ë³µì›í–ˆëŠ”ê°€ì— ëŒ€í•œ lossì¸ë“¯ 

  3.  **L perceptual**

     ![image](https://user-images.githubusercontent.com/26568793/62412156-9a528080-b639-11e9-8fc6-292653b2c24a.png)

     - I out : ëª¨ë¸ì„ íƒ€ê³  ë‚˜ì˜¤ëŠ” ê²°ê³¼ ì´ë¯¸ì§€

     - I comp : I outì—ì„œ non-holeë¶€ë¶„ì„ ì›ë˜ input pixelê°’ìœ¼ë¡œ ë°”ê¾¼ ê²ƒ (I outê²½ìš°ì—ëŠ” non holeë¶€ë¶„ì´ ë°”ë€” ìˆ˜ ìˆì–´ì„œë¡œ ë³´ì„)

       => ë‘ í”½ì…€ê°„ì˜ L1 loss, ê·¸ëŸ¬ë‹ˆê¹Œ ì ˆëŒ€ê°’ì˜ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ

       

     CNN(VGG-16)ì˜ POOL, POOL2, POOL3ì˜ activation mapì„ ì‚¬ìš©í•˜ì—¬ lossë¥¼ ê³„ì‚°.

     ```python
     def perceptual_loss(h_comp, h_out, h_gt, l1):
     	loss = 0.0
     
     	for i in range(len(h_comp)):
              # perceptual lossëŠ” style lossì™€ ë‹¤ë¥´ê²Œ gram ì‹ì„ ê±°ì¹˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë°”ë¡œ h_out, h_gt, h_compë¥¼ í†µí•´ l1ë¡œìŠ¤ êµ¬í•¨.
     		loss += l1(h_out[i], h_gt[i])
             # l1 = nn.L1Loss : Mean absolute loss
     		loss += l1(h_comp[i], h_gt[i]) 
      
     	return loss
     ```

     

  4.  **L style**

     ![image](https://user-images.githubusercontent.com/26568793/62412171-b81fe580-b639-11e9-82bd-1d62cc6ed77e.png)

     kn: normalization factor for the nth selected layerê°œë…ì ìœ¼ë¡œ ë³´ë©´ ëª¨ë¸ì´ ë§Œë“¤ì–´ë‚¸ ì´ë¯¸ì§€ë¥¼ í•™ìŠµëœ vggëª¨ë¸ì— íƒœì›Œì„œ ë§Œë“¤ì–´ë‚¸ ì´ë¯¸ì§€ì˜ styleì„ í•™ìŠµí•˜ê³  ì›ë˜ ground truth ì´ë¯¸ì§€ë¥¼ íƒœì›Œì„œ ë§Œë“¤ì–´ë‚¸ ì´ë¯¸ì§€ì˜ styleê³¼ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ í•™ìŠµí•œë‹¤.ì—¬ê¸°ë„ ë§ˆì°¬ê°€ì§€ë¡œ composition ë¶€ë¶„ì„ ë”°ë¡œ í•œë²ˆ ë” lossê³¼ì •ì„ ê±°ì¹˜ëŠ”ë°, ì›ë˜ì˜ outputì´ë¯¸ì§€ì— non holeë¶€ë¶„ì€ gtë¡œ ë°”ê¾¼ ë¶€ë¶„ì„ í•™ìŠµëœ vggê±°ì¹œ ê²ƒê³¼ gtê°€ ê±°ì¹œ ê²ƒê³¼ì˜ ì°¨ì´ë¥¼ í•™ìŠµí•œë‹¤.
     => ì—¬ê¸°ì„œ pëŠ” layerì˜ í•œ ì¸µì„ ì˜ë¯¸í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.CpxCp : Gram matrix   Kp :normalization factor 1 /CpHpWp  for pth selected layer 

     ```python
     # ëª¨ë¸ì´ ë§Œë“  ì´ë¯¸ì§€ë¥¼ í•™ìŠµëœ VGGëª¨ë¸ì— íƒœì›Œì„œ ë§Œë“¤ì–´ë‚¸ ì´ë¯¸ì§€ styleì„ í•™ìŠµí•˜ê³  
     # ì›ë˜ groud_truth ì´ë¯¸ì§€ë¥¼ íƒœì›Œì„œ ë§Œë“¤ì–´ë‚¸ ì´ë¯¸ì§€ì˜ styleê³¼ì˜ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ í•™ìŠµ
     # style_loss(fs_composed_output, fs_output, fs_ground_truth, self.l1) * LAMBDAS["style"] 
     # ì´ë•Œ fs_~ ë¥¼ ëª¨ë¥´ë‹ˆê¹Œ class CalculateLossì˜ forwardë¥¼ ë¨¼ì € ë³´ì.
     
     def style_loss(h_comp, h_out, h_gt, l1): # h_comp : fs_composed_output, h_out : fs_output, h_gt : fs_ground_truth, l1 : self.l1 
     	loss = 0.0
     
     	for i in range(len(h_comp)):
     		loss += l1(gram_matrix(h_out[i]), gram_matrix(h_gt[i])) # K_n ê°’ì„ gram_matrixì—ì„œ ë°”ë¡œ êµ¬í–ˆìœ¼ë‹ˆ L_style_outì˜ lossë¥¼ ë°”ë¡œ êµ¬í•¨
     		loss += l1(gram_matrix(h_comp[i]), gram_matrix(h_gt[i])) #K_n ê°’ì„ gram_matrixì—ì„œ ë°”ë¡œ êµ¬í–ˆìœ¼ë‹ˆ L_style_compì˜ lossë¥¼ ë°”ë¡œ êµ¬í•¨
                                                                 # L_style_out + L_style_compëŠ” ê°€ì¤‘ì¹˜ê°€ ë‘˜ë‹¤ 120ì´ë¯€ë¡œ í•œë²ˆì— ë”í•´ì„œ lossë¡œ ë°˜í™˜
     	return loss
     
     def gram_matrix(feature_matrix): # function style_loss ì™€ ê°™ì´ ë³´ì.
     	(batch, channel, h, w) = feature_matrix.size() #feature_matrix í…ì„œ shapeì„ ë°˜í™˜
     	feature_matrix = feature_matrix.view(batch, channel, h * w) # viewëŠ” reshapeê³¼ ê°™ì€ í•¨ìˆ˜ ì¦‰ batch x channel x h*wë¥¼ ê°–ëŠ” shapeìœ¼ë¡œ ë³€í™˜
     	feature_matrix_t = feature_matrix.transpose(1, 2) # transposeëŠ” ì „ì¹˜, ë‘ë²ˆ ì§¸ ì°¨ì›ê³¼ ì„¸ë²ˆ ì§¸ ì°¨ì› ë°”ê¿”ì¤Œ.
     
     	# batch matrix multiplication * normalization factor K_n
     	# (batch, channel, h * w) x (batch, h * w, channel) ==> (batch, channel, channel)
     	gram = torch.bmm(feature_matrix, feature_matrix_t) / (channel * h * w)  # bmmì€ batch matrix-matrix product, ì¦‰ í–‰ë ¬ ê³±
                                                                               # ë…¼ë¬¸ ì •ì˜ë¥¼ ë³´ë©´ K_nì´ (1/channel * h*w)ë¡œ normalize factor -> ì´ ë¶€ë¶„ì´ K_n
     
     	# size = (batch, channel, channel)
     	return gram # gram ë°˜í™˜
     ```

     

  5.  **L tv: ê²½ê³„**

     ë©´ì˜ ì°¨ì´ë¥¼ ì¤„ì´ë„ë¡ í•™ìŠµí•˜ëŠ” loss (total variation loss)    holeì¸ ë¶€ë¶„ì—ì„œ ê°€ì¥ ì¸ì ‘í•´ìˆëŠ” ê²½ê³„ë§Œ ë³´ëŠ” ê²ƒ     P: holeì¸ ë¶€ë¶„ì—ì„œ non-holeê³¼ ë¶™ì–´ ìˆëŠ” ê²½ê³„ 

     - ì‚¼ì§€ì°½ê°™ì€ ê¸°í˜¸ëŠ” activation mapì„ ì”Œì›Œì„œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ë¥¼ í‘œí˜„í•œ ê¸°í˜¸ë¼ ë³´ë©´ ë¨. 

       ```python
       # computes TV loss over entire composed image since gradient will not be passed backward to input
       def total_variation_loss(image, l1): # image = composed_output
           # shift one pixel and get loss1 difference (for both x and y direction)
           # image[batch, channel, h,w]
           
           loss = l1(image[:, :, :, :-1] - image[:, :, :, 1:]) + l1(image[:, :, :-1, :] - image[:, :, 1:, :])
           
           # [::::] í–‰ë ¬ í¸ì§‘ ì˜ˆì œ
           """
           ex) a = torch.range(1,16)
               a = view(2,2,4) 2x4 í–‰ë ¬ 2ê°œì¸ 3ì°¨ì› í–‰ë ¬
               
               a[0] : 1 2 3 4   a[1]  : 9 10 11 12
                      5 6 7 8          13 14 15 16
               
               b = a[:,:,:-1]
               b.shape : [2,2,3] 2x3 í–‰ë ¬ 2ê°œì¸ 3ì°¨ì› í–‰ë ¬  
               b[0] : 1 2 3    b[1]  : 9 10 11 
                      5 6 7           13 14 15         
               
               c = a[:,:,1:]
               c.shape : [2,2,3] 2x3 í–‰ë ¬ 2ê°œì¸ 3ì°¨ì› í–‰ë ¬  
               c[0] :  2 3 4    c[1]  :  10 11 12 
                       6 7 8             14 15 16         
                       
               d = a[:,:-1,:]
               d.shape : [2,1,4] 1x4 í–‰ë ¬ 2ê°œì¸ 3ì°¨ì› í–‰ë ¬  
               d[0] : 1 2 3 4   d[1]  : 9 10 11 12
               
               e = a[:,1:,:]
               e.shape : [2,1,4] 1x4 í–‰ë ¬ 2ê°œì¸ 3ì°¨ì› í–‰ë ¬  
               e[0] : 5 6 7 8   e[1]  : 13 14 15 16                         
                                
           """
           return loss
       ```

       ë‚˜ë¨¸ì§€ ì½”ë“œ 

       ```python
       class CalculateLoss(nn.Module):
       	def __init__(self):
       		super().__init__()
       		self.vgg_extract = VGG16Extractor() #í•¨ìˆ˜ VGG16Extractor() ë°˜í™˜ 
       		self.l1 = nn.L1Loss() # L1 ë¡œìŠ¤ ì‚¬ìš©
       
       	def forward(self, input_x, mask, output, ground_truth): 
       		composed_output = (input_x * mask) + (output * (1 - mask)) # compose_output ì •ì˜ I_comp
       
       		fs_composed_output = self.vgg_extract(composed_output) # í¬ì‚¬ì´(I_comp)
       		fs_output = self.vgg_extract(output)                   # í¬ì‚¬ì´(I_out)
       		fs_ground_truth = self.vgg_extract(ground_truth)       # í¬ì‚¬ì´(I_gt)
       
       		loss_dict = dict() # key- value ë¥¼ ë°›ëŠ” dictë¡œ loss_dict ì„ ì–¸
       
       		loss_dict["hole"] = self.l1((1 - mask) * output, (1 - mask) * ground_truth) * LAMBDAS["hole"]
       		loss_dict["valid"] = self.l1(mask * output, mask * ground_truth) * LAMBDAS["valid"]
       		loss_dict["perceptual"] = perceptual_loss(fs_composed_output, fs_output, fs_ground_truth, self.l1) * LAMBDAS["perceptual"]
       		loss_dict["style"] = style_loss(fs_composed_output, fs_output, fs_ground_truth, self.l1) * LAMBDAS["style"]
       		loss_dict["tv"] = total_variation_loss(composed_output, self.l1) * LAMBDAS["tv"]
       
       		return loss_dict
       
       
       # Unit Test
       if __name__ == '__main__':
       	cwd = os.getcwd() # í˜„ì¬ ìœ„ì¹˜ ë°˜í™˜
       	loss_func = CalculateLoss() #calculateLoss í•¨ìˆ˜ í˜¸ì¶œí•˜ê³  returnìœ¼ë¡œ 
       
       	gt = Image.open(cwd + "/test_256/Places365_test_00000050.jpg") # ground truth ì´ë¯¸ì§€ í˜¸ì¶œ
       	mask = Image.open(cwd + "/mask/mask_512.jpg") # mask ì´ë¯¸ì§€ í˜¸ì¶œ
       
       	img_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(MEAN, STDDEV)]) # í…ì„œ ë³€í™˜, ë…¸ë§ë¼ì´ì¦ˆ
        	mask_transform = transforms.ToTensor() #í…ì„œë³€í™˜ë§Œ
       
       	gt = img_transform(gt.convert("RGB")) #ground truth ì´ë¯¸ì§€ë¥¼   rgbì±„ë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ í…ì„œë³€í™˜, ë…¸ë§ë¼ì´ì¦ˆ
       	mask = img_transform(mask.convert("RGB")) # mask ì´ë¯¸ì§€ë¥¼ rgbì±„ë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ í…ì„œ ë³€í™˜í›„ 
       	img = gt * mask # imgëŠ” ground truth * mask
       
       	img.unsqueeze_(0) #ì°¨ì› ì‚½ì…
       	mask.unsqueeze_(0)#ì°¨ì› ì‚½ì…
       	gt.unsqueeze_(0)#ì°¨ì› ì‚½ì…
       
       	loss_out = loss_func(mask, img, gt)
       
       	for key, value in loss_out.items():
       		print("KEY:{} | VALUE:{}".format(key, value))
       ```

       

#### 4.1 Irregular Mask Dataset 

- ì‹¤í—˜ì  íŠ¹ì´ì„± (Partial convolutionì„ ì‚¬ìš©í•œ ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ”, ë§ˆìŠ¤í¬ì— ëŒ€í•œ ì‹¤í—˜ê³¼ë„ ê´€ë ¨ì´ ìˆì–´ë³´ì¸ë‹¤.)
  - 55,116 masks ë°ì´í„°ì…‹ for training , 24,866 for testing, random dilation, rotation and cropping , 512 x 512 
  - ê¸°ì¤€ë§ˆìŠ¤í¬ë“¤ë„ ì¢…ë¥˜ê°€ ìˆìŒ, ë¨¼ì € êµ¬ë©ì´ ê²½ê³„ì„ ê³¼ ë©€ë¦¬ ë–¨ì–´ì ¸ ë‚˜ìˆëŠ” ê²ƒ, ê°€ê¹Œì´ì— ê°™ì´ ëš«ë ¤ ìˆëŠ” ê²ƒ.

#### 4.2 Training Process

- datasetì„¤ëª… - ë…¼ë¬¸ ì°¸ê³ 
- batch size 6, V100 single, imagenet classification initializerë¥¼ ì‚¬ìš©í•œ ê²ƒìœ¼ë¡œ ë³´ì„
- Batch normalization Issue:  ë°°ì¹˜ë§ˆë‹¤ ì „ì²´ í”½ì…€ì— ëŒ€í•´ì„œ í‰ê· ê³¼ ë¶„ì‚°ì„ ë§ì¶°ì£¼ê²Œ ë˜ëŠ”ë° êµ¬ë©ë“¤ì´ ì´ ë¶€ë¶„ì— í˜¼ë€ì„ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸ì— ë¬´ì‹œí–ˆë‹¤ê³  ë˜ì–´ìˆìŒ. í•˜ì§€ë§Œ êµ¬ë©ë“¤ì´ ì ì°¨ ì±„ì›Œì§€ê¸° ë•Œë¬¸ì— okay!

